{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробности по ДЗ:\n",
    "1. Нужно реализовать флаг make_downsample - увеличивает кол-во фильтровать вдвое, размер уменьшается вдвое по высоте и ширине - например (64, 16, 16) -> (128, 8, 8)\n",
    "2. Нужно реализовать флаг use_skip_connection - если он включен, то на выходе блока добавляется X со входа - иначе блок работает как обычная сесть\n",
    "\n",
    "Особенности downsample\n",
    "1. Уменьшать размер входного изображения надо посредством conv3x3 со stride=2\n",
    "2. Уменьшаться размер должен первой конволюцией 3x3 в блоке\n",
    "3. В Bottleneck версии - кол-во фильтров меняется первым bottleneck слоем\n",
    "4. В случае если нет флага use_skip_connection не нужно использовать слой для downsample исходного изображения, так как оно не будет дальше использоваться\n",
    "\n",
    "\n",
    "Общие рекомендации по построению ResNet сетей:\n",
    "1. После каждой конволюции идет BatchNorm и Relu слои\n",
    "2. В конце ResNet блока после суммирования идет Relu слой\n",
    "3. Конволюциооные слои, включая слои Bottleneck не используют bias (bias=False) - опциональное\n",
    "\n",
    "Блоки строятся на основании статьи https://arxiv.org/abs/1512.03385\n",
    "\n",
    "Tutorial по Pytorch https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import mul\n",
    "from functools import reduce\n",
    "BOTTLENECK_COEF = 4\n",
    "# небольшой код по подсчету памяти и количества параметров в сети\n",
    "MODULES_STAT=[]\n",
    "\n",
    "def module_forward_hook(module, input, output):\n",
    "    weight = module.weight.size() if not isinstance(module, torch.nn.modules.MaxPool2d) else (0, 0, 0, 0)\n",
    "    print(\"Forward\", module)\n",
    "    MODULES_STAT.append((module, output.size(), weight))\n",
    "    \n",
    "def register_hook(module):\n",
    "    for item in module.children(): \n",
    "        if type(item) in [nn.modules.conv.Conv2d, nn.modules.MaxPool2d, nn.modules.Linear]:\n",
    "            print(\"RegisterHook\", item)\n",
    "            item.register_forward_hook(module_forward_hook)\n",
    "                 \n",
    "def features_mem_and_params(input_tenzor):\n",
    "    input_size = input_tenzor.size()\n",
    "    total_param = 0\n",
    "    total_mem =  reduce(mul,(input_size))\n",
    "    print( \"%02d\" % 0,\n",
    "          'INPUT',\n",
    "          \"memory\",\n",
    "          \"%dx%dx%d=%d\" % (input_size[1], input_size[2], input_size[3], reduce(mul,(input_size))),\n",
    "          \"parameters\", \"%dx%dx%d=%d\"%(0, 0, 0 , 0)\n",
    "         ) \n",
    "    for i, stat in enumerate(MODULES_STAT):\n",
    "        module_name = str(stat).split('(')[1]\n",
    "        total_param += reduce(mul,(stat[2]))\n",
    "        total_mem   += reduce(mul,(stat[1]))\n",
    "        \n",
    "        if 'Linear' in module_name:\n",
    "            print( \"%02d\"%(i+1),'FC',\"memory\", \"%dx%d=%d\"%(stat[1][0], stat[1][1], reduce(mul,(stat[1]))),\n",
    "               \"parameters\", \"%dx%d=%d\"%(stat[2][0], stat[2][1] , reduce(mul,(stat[2]))))\n",
    "        else:    \n",
    "            print( \"%02d\"%(i+1),module_name,\"memory\", \"%dx%dx%d=%d\"%(stat[1][1], stat[1][2], stat[1][3], reduce(mul,(stat[1]))),\n",
    "               \"parameters\", \"%dx%dx%dx%d=%d\"%(stat[2][0], stat[2][1], stat[2][2], stat[2][3] , reduce(mul,(stat[2]))))\n",
    "    print()\n",
    "    print (\"Total_mem: %d * 4 = %d\" % (total_mem, total_mem * 4))\n",
    "    print (\"Total params: %d\" % total_param, \"Total_mem: %d\" % total_mem)          \n",
    "    return (total_param, total_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNSAMPLE_COEF = 2\n",
    "\n",
    "def conv3x3(a_in_planes, a_out_planes, a_stride=1):\n",
    "    \"\"\"\n",
    "    Основной строительный блок конволюций для ResNet\n",
    "    Включает в себя padding=1 - чтобы размерность сохранялась после его применения\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(a_in_planes, a_out_planes,  stride=a_stride,\n",
    "                     kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "def x_downsample(a_in_channels):\n",
    "     return nn.Conv2d(a_in_channels, \n",
    "               a_in_channels*DOWNSAMPLE_COEF,\n",
    "               kernel_size=1,\n",
    "               stride=2,\n",
    "               bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fe4d0c8f29a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCifarResidualBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_in_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_downsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_skip_connection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCifarResidualBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_skip_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_skip_connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_downsample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_downsample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class CifarResidualBlock(nn.Module):\n",
    "    def __init__(self, a_in_channels, make_downsample=False, use_skip_connection=True):\n",
    "        super(CifarResidualBlock, self).__init__()\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        self.make_downsample = make_downsample\n",
    "        \n",
    "        if make_downsample: coef = DOWNSAMPLE_COEF\n",
    "        else: coef = 1  \n",
    "        \n",
    "        self.conv1 = conv3x3(a_in_channels, a_in_channels * coef, a_stride=coef)\n",
    "        self.bn1 = nn.BatchNorm2d(a_in_channels * coef)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = conv3x3(a_in_channels * coef, a_in_channels * coef)\n",
    "        self.bn2 = nn.BatchNorm2d(a_in_channels * coef)      \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv3 = x_downsample(a_in_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(a_in_channels * DOWNSAMPLE_COEF )     \n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "\n",
    "        if self.make_downsample and self.use_skip_connection:\n",
    "            residual = self.conv3(residual)\n",
    "            residual = self.bn3(residual)\n",
    "            residual = self.relu(residual)\n",
    "\n",
    "        if self.use_skip_connection:\n",
    "            out += residual\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarResidualBottleneckBlock(nn.Module):\n",
    "    \n",
    "    BOTTLENECK_COEF = 4\n",
    "    \n",
    "    def __init__(self, a_in_channels, make_downsample=False, use_skip_connection=True):\n",
    "        super(CifarResidualBottleneckBlock, self).__init__()\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        self.make_downsample = make_downsample\n",
    "        \n",
    "        if make_downsample: coef, stride= DOWNSAMPLE_COEF, 2\n",
    "        else: coef, stride = 1, 1\n",
    "                        \n",
    "        self.conv1 = nn.Conv2d(a_in_channels, int(a_in_channels/BOTTLENECK_COEF)*coef, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(int(a_in_channels/BOTTLENECK_COEF)*coef)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(int(a_in_channels/BOTTLENECK_COEF)*coef, int(a_in_channels/BOTTLENECK_COEF)*coef, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(int(a_in_channels/BOTTLENECK_COEF)*coef)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(int(a_in_channels/BOTTLENECK_COEF)*coef, a_in_channels * coef, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(a_in_channels*coef)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv4 = x_downsample(a_in_channels)\n",
    "        self.bn4 = nn.BatchNorm2d(a_in_channels*DOWNSAMPLE_COEF)     \n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "    \n",
    "            \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        print(out.size())\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        print(out.size())\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        print(out.size())\n",
    "        \n",
    "        if self.make_downsample and self.use_skip_connection:\n",
    "            residual = self.conv4(residual)\n",
    "            residual = self.bn4(residual)\n",
    "            residual = self.relu(residual)\n",
    "            print(residual.size())\n",
    "            \n",
    "        if self.use_skip_connection:\n",
    "            out += residual\n",
    "\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size :\t\t torch.Size([1, 3, 32, 32])\n",
      "After first layers:\t torch.Size([1, 16, 32, 32])\n",
      "After ResBlock layers:\t torch.Size([1, 16, 32, 32])\n",
      "torch.Size([1, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "### Test 1\n",
    "x = torch.ones(1, 3, 32, 32)*100\n",
    "print(\"Input size :\\t\\t\", x.size())\n",
    "\n",
    "first_conv = nn.Conv2d(3, 16, 3, padding=1, bias=False)\n",
    "x = first_conv(x)\n",
    "print(\"After first layers:\\t\", x.size())\n",
    "\n",
    "block = CifarResidualBlock(16, make_downsample=False, use_skip_connection=True)\n",
    "x = block(x)\n",
    "print(\"After ResBlock layers:\\t\", x.size())\n",
    "print(x.size())\n",
    "assert(x.size() == torch.Size((1, 16, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size :\t\t torch.Size([1, 3, 32, 32])\n",
      "After first layers:\t torch.Size([1, 16, 32, 32])\n",
      "After ResBlock layers:\t torch.Size([1, 32, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "### Test 2\n",
    "x = torch.ones(1, 3, 32, 32)*100\n",
    "print(\"Input size :\\t\\t\", x.size())\n",
    "\n",
    "first_conv = nn.Conv2d(3, 16, 3, padding=1, bias=False)\n",
    "x = first_conv(x)\n",
    "print(\"After first layers:\\t\", x.size())\n",
    "\n",
    "block = CifarResidualBlock(16, make_downsample=True, use_skip_connection=True)\n",
    "x = block(x)\n",
    "print(\"After ResBlock layers:\\t\", x.size())\n",
    "\n",
    "assert(x.size() == torch.Size((1, 32, 16, 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384 tensor(16953.8184, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### Test 3\n",
    "x = torch.ones(1, 16, 32, 32)\n",
    "block = CifarResidualBlock(16, make_downsample=False, use_skip_connection=True)\n",
    "x = block(x)\n",
    "print(x.size()[1]*x.size()[2]*x.size()[3], x.sum())\n",
    "assert(x.sum() > 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384 tensor(1804.1643, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### Test 4\n",
    "x = torch.ones(1, 16, 32, 32)\n",
    "block = CifarResidualBlock(16, make_downsample=False, use_skip_connection=False)\n",
    "x = block(x)\n",
    "print(x.size()[1]*x.size()[2]*x.size()[3], x.sum())\n",
    "assert(x.sum() < 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegisterHook Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "Forward Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Forward Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Forward Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "00 INPUT memory 256x8x8=16384 parameters 0x0x0=0\n",
      "01 Conv2d memory 512x4x4=8192 parameters 512x256x3x3=1179648\n",
      "02 Conv2d memory 512x4x4=8192 parameters 512x512x3x3=2359296\n",
      "03 Conv2d memory 512x4x4=8192 parameters 512x256x1x1=131072\n",
      "\n",
      "Total_mem: 40960 * 4 = 163840\n",
      "Total params: 3670016 Total_mem: 40960\n",
      "3670016 40960\n"
     ]
    }
   ],
   "source": [
    "### Test 5\n",
    "\n",
    "MODULES_STAT=[]\n",
    "\n",
    "input = torch.ones(1, 256, 8, 8)\n",
    "block = CifarResidualBlock(256, make_downsample=True, use_skip_connection=True)\n",
    "register_hook(block)\n",
    "out = block(input)\n",
    "\n",
    "params, memory = features_mem_and_params(input)\n",
    "print(params, memory)\n",
    "assert((params, memory) == (3670016, 40960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegisterHook Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "Forward Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Forward Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "00 INPUT memory 256x8x8=16384 parameters 0x0x0=0\n",
      "01 Conv2d memory 256x8x8=16384 parameters 256x256x3x3=589824\n",
      "02 Conv2d memory 256x8x8=16384 parameters 256x256x3x3=589824\n",
      "\n",
      "Total_mem: 49152 * 4 = 196608\n",
      "Total params: 1179648 Total_mem: 49152\n",
      "1179648 49152\n"
     ]
    }
   ],
   "source": [
    "### Test 6\n",
    "\n",
    "MODULES_STAT=[]\n",
    "\n",
    "input = torch.ones(1, 256, 8, 8)\n",
    "block = CifarResidualBlock(256, make_downsample=False, use_skip_connection=True)\n",
    "register_hook(block)\n",
    "out = block(input)\n",
    "\n",
    "params, memory = features_mem_and_params(input)\n",
    "print(params, memory)\n",
    "assert((params, memory) == (1179648, 49152))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegisterHook Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "Forward Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Forward Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "00 INPUT memory 256x8x8=16384 parameters 0x0x0=0\n",
      "01 Conv2d memory 512x4x4=8192 parameters 512x256x3x3=1179648\n",
      "02 Conv2d memory 512x4x4=8192 parameters 512x512x3x3=2359296\n",
      "\n",
      "Total_mem: 32768 * 4 = 131072\n",
      "Total params: 3538944 Total_mem: 32768\n",
      "3538944 32768\n"
     ]
    }
   ],
   "source": [
    "### Test 7\n",
    "\n",
    "MODULES_STAT=[]\n",
    "\n",
    "input = torch.ones(1, 256, 8, 8)\n",
    "block = CifarResidualBlock(256, make_downsample=True, use_skip_connection=False)\n",
    "register_hook(block)\n",
    "out = block(input)\n",
    "\n",
    "params, memory = features_mem_and_params(input)\n",
    "print(params, memory)\n",
    "assert((params, memory) == (3538944, 32768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegisterHook Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "Forward Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Forward Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "00 INPUT memory 256x8x8=16384 parameters 0x0x0=0\n",
      "01 Conv2d memory 256x8x8=16384 parameters 256x256x3x3=589824\n",
      "02 Conv2d memory 256x8x8=16384 parameters 256x256x3x3=589824\n",
      "\n",
      "Total_mem: 49152 * 4 = 196608\n",
      "Total params: 1179648 Total_mem: 49152\n",
      "1179648 49152\n"
     ]
    }
   ],
   "source": [
    "### Test 8\n",
    "\n",
    "MODULES_STAT=[]\n",
    "\n",
    "input = torch.ones(1, 256, 8, 8)\n",
    "block = CifarResidualBlock(256, make_downsample=False, use_skip_connection=False)\n",
    "register_hook(block)\n",
    "out = block(input)\n",
    "\n",
    "params, memory = features_mem_and_params(input)\n",
    "print(params, memory)\n",
    "assert((params, memory) == (1179648, 49152))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegisterHook Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "Forward Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "torch.Size([1, 64, 8, 8])\n",
      "Forward Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([1, 64, 8, 8])\n",
      "Forward Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "torch.Size([1, 256, 8, 8])\n",
      "00 INPUT memory 256x8x8=16384 parameters 0x0x0=0\n",
      "01 Conv2d memory 64x8x8=4096 parameters 64x256x1x1=16384\n",
      "02 Conv2d memory 64x8x8=4096 parameters 64x64x3x3=36864\n",
      "03 Conv2d memory 256x8x8=16384 parameters 256x64x1x1=16384\n",
      "\n",
      "Total_mem: 40960 * 4 = 163840\n",
      "Total params: 69632 Total_mem: 40960\n",
      "69632 40960\n"
     ]
    }
   ],
   "source": [
    "### Test 9\n",
    "\n",
    "MODULES_STAT=[]\n",
    "\n",
    "input = torch.ones(1, 256, 8, 8)\n",
    "block = CifarResidualBottleneckBlock(256, make_downsample=False, use_skip_connection=False)\n",
    "register_hook(block)\n",
    "out = block(input)\n",
    "\n",
    "params, memory = features_mem_and_params(input)\n",
    "print(params, memory)\n",
    "assert((params, memory) == (69632, 40960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegisterHook Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "Forward Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "torch.Size([1, 64, 8, 8])\n",
      "Forward Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([1, 64, 8, 8])\n",
      "Forward Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "torch.Size([1, 256, 8, 8])\n",
      "00 INPUT memory 256x8x8=16384 parameters 0x0x0=0\n",
      "01 Conv2d memory 64x8x8=4096 parameters 64x256x1x1=16384\n",
      "02 Conv2d memory 64x8x8=4096 parameters 64x64x3x3=36864\n",
      "03 Conv2d memory 256x8x8=16384 parameters 256x64x1x1=16384\n",
      "\n",
      "Total_mem: 40960 * 4 = 163840\n",
      "Total params: 69632 Total_mem: 40960\n",
      "69632 40960\n"
     ]
    }
   ],
   "source": [
    "### Test 10\n",
    "\n",
    "MODULES_STAT=[]\n",
    "\n",
    "input = torch.ones(1, 256, 8, 8)\n",
    "block = CifarResidualBottleneckBlock(256, make_downsample=False, use_skip_connection=True)\n",
    "register_hook(block)\n",
    "out = block(input)\n",
    "\n",
    "params, memory = features_mem_and_params(input)\n",
    "print(params, memory)\n",
    "assert((params, memory) == (69632, 40960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegisterHook Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "Forward Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "torch.Size([1, 128, 8, 8])\n",
      "Forward Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "torch.Size([1, 128, 4, 4])\n",
      "Forward Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "torch.Size([1, 512, 4, 4])\n",
      "00 INPUT memory 256x8x8=16384 parameters 0x0x0=0\n",
      "01 Conv2d memory 128x8x8=8192 parameters 128x256x1x1=32768\n",
      "02 Conv2d memory 128x4x4=2048 parameters 128x128x3x3=147456\n",
      "03 Conv2d memory 512x4x4=8192 parameters 512x128x1x1=65536\n",
      "\n",
      "Total_mem: 34816 * 4 = 139264\n",
      "Total params: 245760 Total_mem: 34816\n",
      "245760 34816\n"
     ]
    }
   ],
   "source": [
    "### Test 11\n",
    "\n",
    "MODULES_STAT=[]\n",
    "\n",
    "input = torch.ones(1, 256, 8, 8)\n",
    "block = CifarResidualBottleneckBlock(256, make_downsample=True, use_skip_connection=False)\n",
    "register_hook(block)\n",
    "out = block(input)\n",
    "\n",
    "params, memory = features_mem_and_params(input)\n",
    "print(params, memory)\n",
    "assert((params, memory) == (245760, 34816))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegisterHook Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "RegisterHook Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "Forward Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "torch.Size([1, 128, 8, 8])\n",
      "Forward Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "torch.Size([1, 128, 4, 4])\n",
      "Forward Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "torch.Size([1, 512, 4, 4])\n",
      "Forward Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "torch.Size([1, 512, 4, 4])\n",
      "00 INPUT memory 256x8x8=16384 parameters 0x0x0=0\n",
      "01 Conv2d memory 128x8x8=8192 parameters 128x256x1x1=32768\n",
      "02 Conv2d memory 128x4x4=2048 parameters 128x128x3x3=147456\n",
      "03 Conv2d memory 512x4x4=8192 parameters 512x128x1x1=65536\n",
      "04 Conv2d memory 512x4x4=8192 parameters 512x256x1x1=131072\n",
      "\n",
      "Total_mem: 43008 * 4 = 172032\n",
      "Total params: 376832 Total_mem: 43008\n",
      "376832 43008\n"
     ]
    }
   ],
   "source": [
    "### Test 12\n",
    "\n",
    "MODULES_STAT=[]\n",
    "\n",
    "input = torch.ones(1, 256, 8, 8)\n",
    "block = CifarResidualBottleneckBlock(256, make_downsample=True, use_skip_connection=True)\n",
    "register_hook(block)\n",
    "out = block(input)\n",
    "\n",
    "params, memory = features_mem_and_params(input)\n",
    "print(params, memory)\n",
    "assert((params, memory) == (376832, 43008))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
