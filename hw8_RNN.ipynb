{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = 'ololoasdasddqweqw123456789'\n",
    "word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.241093635559082\n",
      "Clip gradient :  2.9952871038773936\n",
      "1.9175242185592651\n",
      "Clip gradient :  1.0000687721702373\n",
      "0.053429603576660156\n",
      "Clip gradient :  0.21417662657747444\n",
      "0.006561756134033203\n",
      "Clip gradient :  0.026923841330445467\n",
      "0.003204822540283203\n",
      "Clip gradient :  0.013663283545834303\n",
      "0.002155303955078125\n",
      "Clip gradient :  0.004036101400384051\n",
      "0.0018548965454101562\n",
      "Clip gradient :  0.00474431845079919\n",
      "0.0016655921936035156\n",
      "Clip gradient :  0.0030348992287075266\n",
      "0.0015568733215332031\n",
      "Clip gradient :  0.002928868256854937\n",
      "0.0014696121215820312\n",
      "Clip gradient :  0.0026265569733631773\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t hello\n",
      "Original:\t hello\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово\n",
    "Сохранить ноутбук с предсказанием и пройденным assert и прислать на почту a.murashev@corp.mail.ru\n",
    "c темой:\n",
    "\n",
    "\n",
    "[МФТИ\\_2019\\_1] ДЗ №8 ФИО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.Wxc = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.Whc = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.Wxi = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.Whi = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.Wxf = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.Whf = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.Wxo = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.Who = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.activation_t  = nn.Tanh()\n",
    "        self.activation_si = nn.Sigmoid()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, h_prev, ct_prev):\n",
    "        ct_ = self.activation_t(self.Wxc(x) + self.Whc(h_prev))\n",
    "        it = self.activation_si(self.Wxi(x) + self.Whi(h_prev))\n",
    "        ft = self.activation_si(self.Wxf(x) + self.Whf(h_prev))\n",
    "        ot = self.activation_si(self.Wxo(x) + self.Who(h_prev))\n",
    "\n",
    "        ct = ft * ct_prev + it * ct_\n",
    "        ht = ot * self.activation_t(ct)\n",
    "        \n",
    "        output = self.outweight(ht)\n",
    "        return output, ht, ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "lstm = LSTM(in_size=ds.vec_size, hidden_size=9, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(lstm.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.27651977539062\n",
      "Clip gradient :  3.2333055592902125\n",
      "64.65594482421875\n",
      "Clip gradient :  4.671078924658394\n",
      "44.599403381347656\n",
      "Clip gradient :  8.193819036041655\n",
      "33.583961486816406\n",
      "Clip gradient :  13.216542360622869\n",
      "22.000526428222656\n",
      "Clip gradient :  7.019817953584795\n",
      "12.808025360107422\n",
      "Clip gradient :  7.344742214919685\n",
      "9.923479080200195\n",
      "Clip gradient :  7.831807558328731\n",
      "6.379441261291504\n",
      "Clip gradient :  6.17292959923518\n",
      "3.757295608520508\n",
      "Clip gradient :  5.158148500315033\n",
      "1.7752985954284668\n",
      "Clip gradient :  2.370055276139136\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(lstm.Whc.in_features)\n",
    "    ct = torch.zeros(lstm.Whc.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh, ct = lstm(x, hh, ct)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=1)\n",
    "            \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "lstm.eval()\n",
    "hh = torch.zeros(lstm.Whc.in_features)\n",
    "ct = torch.zeros(lstm.Whc.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh, ct = lstm(x, hh, ct)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.Wxu = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.Whu = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.Wxr = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.Whr = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.Wxh_ = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.Whh_ = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.activation_t  = nn.Tanh()\n",
    "        self.activation_si = nn.Sigmoid()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        ut = self.activation_t(self.Wxu(x) + self.Whu(h_prev))\n",
    "        rt = self.activation_si(self.Wxr(x) + self.Whr(h_prev))\n",
    "        h_t = self.activation_t(self.Wxh_(x) + self.Whh_(rt * h_prev))\n",
    "        \n",
    "        ht = (torch.ones(ut.size()) - ut) * h_t + ut * h_prev\n",
    "        \n",
    "        output = self.outweight(ht)\n",
    "        return output, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "gru = GRU(in_size=ds.vec_size, hidden_size=9, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 500\n",
    "lr        = 0.01\n",
    "optim     = SGD(gru.parameters(), lr = lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.15213775634766\n",
      "Clip gradient :  5.694520194498311\n",
      "67.86914825439453\n",
      "Clip gradient :  4.728119251044438\n",
      "62.29629898071289\n",
      "Clip gradient :  6.898049172334389\n",
      "53.960670471191406\n",
      "Clip gradient :  11.60842783438284\n",
      "42.48074722290039\n",
      "Clip gradient :  17.097925122630027\n",
      "31.662235260009766\n",
      "Clip gradient :  24.93448841701567\n",
      "24.05257797241211\n",
      "Clip gradient :  37.16940087202575\n",
      "18.845861434936523\n",
      "Clip gradient :  72.69995957023778\n",
      "16.932872772216797\n",
      "Clip gradient :  43.58018625849628\n",
      "14.781505584716797\n",
      "Clip gradient :  106.4866426979809\n",
      "13.825221061706543\n",
      "Clip gradient :  23.87333194149107\n",
      "9.120635986328125\n",
      "Clip gradient :  80.56539492775272\n",
      "11.601009368896484\n",
      "Clip gradient :  132.87296204974623\n",
      "7.797486305236816\n",
      "Clip gradient :  37.10470078667578\n",
      "8.004602432250977\n",
      "Clip gradient :  50.45964464025645\n",
      "Clipped lr\n",
      "7.927195072174072\n",
      "Clip gradient :  95.60890427686239\n",
      "8.688392639160156\n",
      "Clip gradient :  76.7301522032651\n",
      "5.0349578857421875\n",
      "Clip gradient :  25.857105062380043\n",
      "4.002920627593994\n",
      "Clip gradient :  41.448967503737\n",
      "3.182978868484497\n",
      "Clip gradient :  145.32745243932723\n",
      "2.403536319732666\n",
      "Clip gradient :  12.200458620897473\n",
      "1.8757166862487793\n",
      "Clip gradient :  44.378570117585056\n",
      "2.597522735595703\n",
      "Clip gradient :  103.89583004510298\n",
      "5.299383163452148\n",
      "Clip gradient :  1130.5628827330972\n",
      "3.052018404006958\n",
      "Clip gradient :  27.900379521661463\n",
      "1.9506239891052246\n",
      "Clip gradient :  80.55304613195518\n",
      "1.7039070129394531\n",
      "Clip gradient :  36.352188433527616\n",
      "2.919978618621826\n",
      "Clip gradient :  282.82929474041686\n",
      "1.7121882438659668\n",
      "Clip gradient :  16.464131108412396\n",
      "3.615818500518799\n",
      "Clip gradient :  596.9249170848327\n",
      "Clipped lr\n",
      "1.548825740814209\n",
      "Clip gradient :  7.23253545600962\n",
      "0.9970846176147461\n",
      "Clip gradient :  3.889465405403468\n",
      "0.7104220390319824\n",
      "Clip gradient :  3.1735837683484496\n",
      "0.5441193580627441\n",
      "Clip gradient :  1.4880619080790385\n",
      "0.4530611038208008\n",
      "Clip gradient :  3.7460998978509785\n",
      "0.3916501998901367\n",
      "Clip gradient :  3.8821180289768824\n",
      "0.32199811935424805\n",
      "Clip gradient :  1.6580659126806607\n",
      "0.26169729232788086\n",
      "Clip gradient :  2.416937820943804\n",
      "0.21123838424682617\n",
      "Clip gradient :  2.669777256948245\n",
      "0.24516725540161133\n",
      "Clip gradient :  57.17891761017098\n",
      "0.25838184356689453\n",
      "Clip gradient :  8.143156650693877\n",
      "6.338191032409668\n",
      "Clip gradient :  982.3993029664298\n",
      "0.3308877944946289\n",
      "Clip gradient :  35.60336468347289\n",
      "10.485880851745605\n",
      "Clip gradient :  297.80944375392636\n",
      "0.3091297149658203\n",
      "Clip gradient :  6.462012031385104\n",
      "Clipped lr\n",
      "0.2162928581237793\n",
      "Clip gradient :  5.642944628013956\n",
      "0.19115066528320312\n",
      "Clip gradient :  5.9077594171250905\n",
      "0.16003131866455078\n",
      "Clip gradient :  1.521275810107582\n",
      "0.14019536972045898\n",
      "Clip gradient :  0.5275898830170453\n",
      "0.12449073791503906\n",
      "Clip gradient :  0.4590036768428852\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(gru.Whu.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = gru(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 150 == 0 and epoch != 0:\n",
    "        lr = lr / 2\n",
    "        optim = SGD(gru.parameters(), lr = lr, momentum=0.9)\n",
    "        print('Clipped lr')\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(gru.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(gru.parameters(), max_norm=1)\n",
    "            \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "gru.eval()\n",
    "hh = torch.zeros(gru.Whu.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = gru(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
